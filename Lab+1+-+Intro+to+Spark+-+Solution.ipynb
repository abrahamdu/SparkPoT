{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Hello Spark\n",
    "This lab will introduce you to Apache Spark.  It will be written in Python and run in IBM's Data Science Experience environment through a Jupyter notebook.  While you work, it will be valuable to reference the [Apache Spark Documentation](http://spark.apache.org/docs/latest/programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Working with Spark Context\n",
    "Step 1.1 - Invoke the spark context: sc.  The version method will return the working version of Apache Spark<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1 - Check spark version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Working with Resilient Distributed Datasets\n",
    "\n",
    "Step 2.1 - Create an RDD with numbers 1 to 10\n",
    "\n",
    "Type: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; x_nbr_rdd = sc.parallelize(x)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Step 2.1 - Create RDD of numbers 1-10\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  #One could also write x = range(1,11)\n",
    "x_nbr_rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.2 - Return the first element<br><br>\n",
    "Type: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.2 - Return first element\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.3 - Return an array of the first five elements<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.3 - Return an array of the first five elements\n",
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.4 - Perform a map transformation to increment each element of the array.  The map function creates a new RDD by applying the function provided in the argument to each element.  For more information go to [Transformations](http://spark.apache.org/docs/latest/programming-guide.html#transformations)<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 2.4 - Write your map function\n",
    "x_nbr_rdd_2=x_nbr_rdd.map(lambda x: x+1)  #It's not required to be x.  (lambda a: a+1) would also work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.5 - Note that there was no result for step 2.4.  Why was this?  Take a look at all the elements of the new RDD.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; x_nbr_rdd_2.collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.5 - Check out the elements of the new RDD. Warning: Be careful with this in real life! Collect returns everything!\n",
    "x_nbr_rdd_2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.6 - Create a new RDD with one string \"Hello Spark\" and print it.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; y_str_rdd = sc.parallelize(y)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; y_str_rdd.first()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Spark'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.6 - Create String RDD, Extract first line\n",
    "y = ['Hello Spark'] #Remember that parallelize takes an iterable such an array.  If you provide only a string, it will iterate through the characters of the string\n",
    "y_str_rdd=sc.parallelize(y)\n",
    "y_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.7 - Create a third RDD with several strings.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z = ['IBM Data Science Experience is built for enterprise-scale deployment.', \"Manage your data, your analytical assets, and your projects in a secured cloud environment.\" , \"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\"]<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd = sc.parallelize(z)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IBM Data Science Experience is built for enterprise-scale deployment.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.7 - Create String RDD with many lines / entries, Extract first line\n",
    "z = 'IBM Data Science Experience is built for enterprise-scale deployment.', \"Manage your data, your analytical assets, and your projects in a secured cloud environment.\" , \"When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.\"\n",
    "z_str_rdd = sc.parallelize(z)\n",
    "z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.8 - Count the number of entries in this RDD.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.8 - Count the number of entries in the RDD\n",
    "z_str_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.9 - Inspect the elements of this RDD.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;z_str_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IBM Data Science Experience is built for enterprise-scale deployment.',\n",
       " 'Manage your data, your analytical assets, and your projects in a secured cloud environment.',\n",
       " 'When you create an account in the IBM Data Science Experience, we deploy for you a Spark as a Service instance to power your analysis and 5 GB of IBM Object Storage to store your data.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.9 - Show all the entries in the RDD\n",
    "z_str_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.10 - Split all the entries in the RDD on the spaces.  Then print it out.  Pay careful attention to the new format.<br><br>\n",
    "Type: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;z_str_rdd_split = z_str_rdd.map(lambda line: line.split(\" \"))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;z_str_rdd_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['IBM',\n",
       "  'Data',\n",
       "  'Science',\n",
       "  'Experience',\n",
       "  'is',\n",
       "  'built',\n",
       "  'for',\n",
       "  'enterprise-scale',\n",
       "  'deployment.'],\n",
       " ['Manage',\n",
       "  'your',\n",
       "  'data,',\n",
       "  'your',\n",
       "  'analytical',\n",
       "  'assets,',\n",
       "  'and',\n",
       "  'your',\n",
       "  'projects',\n",
       "  'in',\n",
       "  'a',\n",
       "  'secured',\n",
       "  'cloud',\n",
       "  'environment.'],\n",
       " ['When',\n",
       "  'you',\n",
       "  'create',\n",
       "  'an',\n",
       "  'account',\n",
       "  'in',\n",
       "  'the',\n",
       "  'IBM',\n",
       "  'Data',\n",
       "  'Science',\n",
       "  'Experience,',\n",
       "  'we',\n",
       "  'deploy',\n",
       "  'for',\n",
       "  'you',\n",
       "  'a',\n",
       "  'Spark',\n",
       "  'as',\n",
       "  'a',\n",
       "  'Service',\n",
       "  'instance',\n",
       "  'to',\n",
       "  'power',\n",
       "  'your',\n",
       "  'analysis',\n",
       "  'and',\n",
       "  '5',\n",
       "  'GB',\n",
       "  'of',\n",
       "  'IBM',\n",
       "  'Object',\n",
       "  'Storage',\n",
       "  'to',\n",
       "  'store',\n",
       "  'your',\n",
       "  'data.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.10 - Perform a map transformation to split all entries in the RDD\n",
    "#Check out the entries in the new RDD\n",
    "z_str_rdd_split = z_str_rdd.map(lambda line: line.split(\" \"))\n",
    "z_str_rdd_split.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.11 - Explore a new transformation: flatMap <br>\n",
    "flatMap will \"flatten\" all the elements of an RDD element into 0 or more output terms.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\" \"))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; z_str_rdd_split_flatmap.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IBM',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Experience',\n",
       " 'is',\n",
       " 'built',\n",
       " 'for',\n",
       " 'enterprise-scale',\n",
       " 'deployment.',\n",
       " 'Manage',\n",
       " 'your',\n",
       " 'data,',\n",
       " 'your',\n",
       " 'analytical',\n",
       " 'assets,',\n",
       " 'and',\n",
       " 'your',\n",
       " 'projects',\n",
       " 'in',\n",
       " 'a',\n",
       " 'secured',\n",
       " 'cloud',\n",
       " 'environment.',\n",
       " 'When',\n",
       " 'you',\n",
       " 'create',\n",
       " 'an',\n",
       " 'account',\n",
       " 'in',\n",
       " 'the',\n",
       " 'IBM',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'Experience,',\n",
       " 'we',\n",
       " 'deploy',\n",
       " 'for',\n",
       " 'you',\n",
       " 'a',\n",
       " 'Spark',\n",
       " 'as',\n",
       " 'a',\n",
       " 'Service',\n",
       " 'instance',\n",
       " 'to',\n",
       " 'power',\n",
       " 'your',\n",
       " 'analysis',\n",
       " 'and',\n",
       " '5',\n",
       " 'GB',\n",
       " 'of',\n",
       " 'IBM',\n",
       " 'Object',\n",
       " 'Storage',\n",
       " 'to',\n",
       " 'store',\n",
       " 'your',\n",
       " 'data.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.11 - Learn the difference between two transformations: map and flatMap.\n",
    "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "z_str_rdd_split_flatmap.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.12 - Augment each entry in the previous RDD with the number \"1\" to create pairs or tuples. The first element of the tuple will be the word and the second elements of the tuple will be the digit \"1\".  This is a common step in performing a count.<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; countWords.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IBM', 1),\n",
       " ('Data', 1),\n",
       " ('Science', 1),\n",
       " ('Experience', 1),\n",
       " ('is', 1),\n",
       " ('built', 1),\n",
       " ('for', 1),\n",
       " ('enterprise-scale', 1),\n",
       " ('deployment.', 1),\n",
       " ('Manage', 1),\n",
       " ('your', 1),\n",
       " ('data,', 1),\n",
       " ('your', 1),\n",
       " ('analytical', 1),\n",
       " ('assets,', 1),\n",
       " ('and', 1),\n",
       " ('your', 1),\n",
       " ('projects', 1),\n",
       " ('in', 1),\n",
       " ('a', 1),\n",
       " ('secured', 1),\n",
       " ('cloud', 1),\n",
       " ('environment.', 1),\n",
       " ('When', 1),\n",
       " ('you', 1),\n",
       " ('create', 1),\n",
       " ('an', 1),\n",
       " ('account', 1),\n",
       " ('in', 1),\n",
       " ('the', 1),\n",
       " ('IBM', 1),\n",
       " ('Data', 1),\n",
       " ('Science', 1),\n",
       " ('Experience,', 1),\n",
       " ('we', 1),\n",
       " ('deploy', 1),\n",
       " ('for', 1),\n",
       " ('you', 1),\n",
       " ('a', 1),\n",
       " ('Spark', 1),\n",
       " ('as', 1),\n",
       " ('a', 1),\n",
       " ('Service', 1),\n",
       " ('instance', 1),\n",
       " ('to', 1),\n",
       " ('power', 1),\n",
       " ('your', 1),\n",
       " ('analysis', 1),\n",
       " ('and', 1),\n",
       " ('5', 1),\n",
       " ('GB', 1),\n",
       " ('of', 1),\n",
       " ('IBM', 1),\n",
       " ('Object', 1),\n",
       " ('Storage', 1),\n",
       " ('to', 1),\n",
       " ('store', 1),\n",
       " ('your', 1),\n",
       " ('data.', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.12 - Create pairs or tuple RDD and print it.\n",
    "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\n",
    "countWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.13 Now we have above what is known as a [Pair RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions). Each entry in the RDD has a KEY and a VALUE.<br>\n",
    "The KEY is the word (Light, of, the, ...) and the value is the number \"1\".  \n",
    "We can now AGGREGATE this RDD by summing up all the values BY KEY<br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;countWords2 = countWords.reduceByKey(lambda x,y: x+y)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;countWords2.collect()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 2),\n",
       " ('enterprise-scale', 1),\n",
       " ('Service', 1),\n",
       " ('is', 1),\n",
       " ('Storage', 1),\n",
       " ('assets,', 1),\n",
       " ('as', 1),\n",
       " ('cloud', 1),\n",
       " ('for', 2),\n",
       " ('secured', 1),\n",
       " ('5', 1),\n",
       " ('you', 2),\n",
       " ('Data', 2),\n",
       " ('store', 1),\n",
       " ('we', 1),\n",
       " ('power', 1),\n",
       " ('When', 1),\n",
       " ('Experience', 1),\n",
       " ('Spark', 1),\n",
       " ('projects', 1),\n",
       " ('a', 3),\n",
       " ('account', 1),\n",
       " ('analysis', 1),\n",
       " ('deployment.', 1),\n",
       " ('analytical', 1),\n",
       " ('the', 1),\n",
       " ('create', 1),\n",
       " ('data,', 1),\n",
       " ('data.', 1),\n",
       " ('IBM', 3),\n",
       " ('instance', 1),\n",
       " ('Science', 2),\n",
       " ('Experience,', 1),\n",
       " ('Manage', 1),\n",
       " ('an', 1),\n",
       " ('to', 2),\n",
       " ('environment.', 1),\n",
       " ('Object', 1),\n",
       " ('GB', 1),\n",
       " ('deploy', 1),\n",
       " ('in', 2),\n",
       " ('of', 1),\n",
       " ('your', 5),\n",
       " ('built', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2.13 - Check out the results of the aggregation\n",
    "countWords2 = countWords.reduceByKey(lambda x,y: x+y)\n",
    "countWords2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Reading a file and counting words<br>\n",
    "<br>\n",
    "Step 3.1 - Read the Apache Spark Readme.md file from Github.  The ! gives you file system commands<br><br>\n",
    "Type:<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;!rm README.md* -f<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;!wget https://raw.githubusercontent.com/apache/spark/master/README.md<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-15 18:24:20--  https://raw.githubusercontent.com/apache/spark/master/README.md\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3828 (3.7K) [text/plain]\n",
      "Saving to: 'README.md'\n",
      "\n",
      "100%[======================================>] 3,828       --.-K/s   in 0s      \n",
      "\n",
      "2016-09-15 18:24:20 (42.8 MB/s) - 'README.md' saved [3828/3828]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 3.1 - Pull data file into workbench\n",
    "\n",
    "!rm README.md* -f\n",
    "!wget https://raw.githubusercontent.com/apache/spark/master/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.2 - Create an RDD by reading from the local filesystem.  Here is the [textfile documentation](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textfile#pyspark.SparkContext.textFile).  Print the count to check that the read was successful.<br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;textfile_rdd = sc.textFile(\"README.md\")<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;textfile_rdd.count()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3.2 - Create RDD from data file\n",
    "textfile_rdd = sc.textFile(\"README.md\")\n",
    "textfile_rdd.count() #You should see 99 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.3<br>Filter out lines that contain \"Spark\". This will be achieved using the [filter](http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=filter#pyspark.RDD.filter) transformation.  Python allows us to use the 'in' syntax to search strings.<br>\n",
    "We will also take a look at the first line in the newly filtered RDD. <br><br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Spark_lines.first()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'# Apache Spark'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3.3 - Filter for only lines with word Spark\n",
    "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\n",
    "Spark_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.4 - Count the number of entries in this filtered RDD and print the result as a concatenated string.<br>\n",
    "Type:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;print \"The file README.md has \" + str(Spark_lines.count()) + \\<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\" of \" + str(textfile_rdd.count()) + \\<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\" lines with the word Spark in it.\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file README.md has 19 of 99 lines with the word Spark in it.\n"
     ]
    }
   ],
   "source": [
    "#Step 3.4 - count the number of lines\n",
    "print \"The file README.md has \" + str(Spark_lines.count()) + \\\n",
    "\" of \" + str(textfile_rdd.count()) + \\\n",
    "\" lines with the word Spark in it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3.5 - Now count the number of times the word Spark appears in the original text, not just the number of lines that contain it.  <br>\n",
    "Instructions:<br>\n",
    "Looking back at previous exercises, you will need to: <br>\n",
    "1 - Execute a flatMap transformation on the original RDD Spark_lines and split on white space.<br>\n",
    "2 - Filter out all instances of the word Spark<br>\n",
    "3 - Count all instances\n",
    "4 - Print the total count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "#Step 3.5 - Count the number of instances of tokens starting with \"Spark\"\n",
    "spark_flat=Spark_lines.flatMap(lambda x: x.split(' '))\n",
    "spark_filtered = spark_flat.filter(lambda word: word == \"Spark\")\n",
    "print spark_filtered.count()\n",
    "#Why is this count different? This count doesn't include where spark was not it's own token such as apache.spark because we only split on whitespace.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Perform analysis on a data file\n",
    "This part is a little more open ended and there are a few ways to complete it.  Scroll up to previous examples for some guidance.  You will download a data file, transform the data, and then average the prices.  The data file will be a sample of tech stock prices over six days. <br>\n",
    "\n",
    "Data Location: https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv<br>\n",
    "The data file is a csv<br>\n",
    "Here is a sample of the file:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"IBM\",\"159.720001\" ,\"159.399994\" ,\"158.880005\",\"159.539993\", \"159.550003\", \"160.350006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-15 18:24:21--  https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 244 [text/plain]\n",
      "Saving to: 'StockPrices.csv'\n",
      "\n",
      "100%[======================================>] 244         --.-K/s   in 0s      \n",
      "\n",
      "2016-09-15 18:24:21 (73.2 MB/s) - 'StockPrices.csv' saved [244/244]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 4.1 - Delete the file if it exists, download a new copy and load it into an RDD\n",
    "!rm StockPrices.csv\n",
    "!wget https://raw.githubusercontent.com/JosephKambourakisIBM/SparkPoT/master/StockPrices.csv\n",
    "SP = sc.textFile('StockPrices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'IBM', 159.720001, 159.399994, 158.880005, 159.539993, 159.550003),\n",
       " (u'MSFT', 58.099998, 57.889999, 57.459999, 57.59, 57.669998),\n",
       " (u'AAPL', 106.82, 106.0, 106.099998, 106.730003, 107.730003),\n",
       " (u'ORCL', 41.310001, 41.310001, 41.220001, 41.16, 41.25)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4.2 - Transform the data to extract the stock ticker symbol and the prices.  \n",
    "SP_mapped = SP.map(lambda line: line.split(','))\n",
    "SP_Keyed = SP_mapped.map(lambda x: (x[0], float(x[1]), float(x[2]), float(x[3]), float(x[4]), float(x[5])))\n",
    "SP_Keyed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'IBM', 159.4179992),\n",
       " (u'MSFT', 57.7419988),\n",
       " (u'AAPL', 106.6760008),\n",
       " (u'ORCL', 41.2500006)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 4.3 - Compute the averages and print them for each symbol.\n",
    "SP_Mean = SP_Keyed.map(lambda x: (x[0], (x[1]+x[2]+ x[3]+ x[4]+x[5])/5)) #There are other ways to do this such as importing a mean function from numpy\n",
    "SP_Mean.collect()\n",
    "\n",
    "#Using a mean function\n",
    "#import numpy\n",
    "#SP_Mean = SP_Keyed.map(lambda x: (x[0], numpy.mean([x[1],x[2], x[3], x[4], x[5]]))) \n",
    "#SP_Mean.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}